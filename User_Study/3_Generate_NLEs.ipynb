{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prompts & parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'prompts_user_study.csv'\n",
    "#filename = 'NLEs_user_study.csv'    # Uncomment if you run the file twice and want to generate NLEs with and without LIME-rationales.\n",
    "prompts = pd.read_csv(filename)\n",
    "\n",
    "# Read the JSON file and parse it into a Python dictionary\n",
    "with open('params_LLM.json', 'r') as file:\n",
    "    params = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select model and setup client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = params['LLM']['key']\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "model = params['LLM']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction = \"You are a helpful assistant.\"\n",
    "instruction_rationale = \"\"\"Your task is to explain the output of a Support Vector Machine for a user. The task is a text classification task.\\n\n",
    "            You will be provided with the input to the model, the model output, confidence_score, and LIME rationales for the model output.\\n\\n\n",
    "            \n",
    "            The input is denoted as ”Input:” and is a string of text.\\n\n",
    "            The model prediction is denoted as ”Output:” and can be one out of three classes [politics, leisure, science].\\n\n",
    "            The confidence_score is denoted as ”Confidence:” and is a value between 0 and 1. A value closer to 1 indicates that the model is more confident in its prediction.\\n\n",
    "\n",
    "            The LIME rationales are extracted by a LIME model. It provides the top-k most influential input features for the model decision in a ranked order. The first feature is the most influential.\\n\n",
    "            The format of a LIME rationale looks like this [feature: value].\\n\n",
    "            If a feature value is positive, it is rational for the predicted class. If the feature value is negative (starts with ”-”), it is a rationale against the predicted class.\\n\\n\n",
    "            \n",
    "            A LIME rationale for the output is denoted as ”Output rationale:” and looks like: [motorcycles: 0.358, BMW: 0.309, used: -0.144, known: -0.131, twin: 0.119, non: -0.097]\\n\\n\n",
    "            \n",
    "            Generate an explanation of the model prediction without mentioning LIME.\\n\n",
    "            Keep it short and structure it in a way that is easily comprehensible.\"\"\"\n",
    "\n",
    "instruction_no_rationale = \"\"\"Your task is to explain the output of a Support Vector Machine for a user. The task is a text classification task.\\n\n",
    "            You will be provided with the input to the model, the model output, and confidence_score for the model output.\\n\\n\n",
    "            \n",
    "             The input is denoted as ”Input:” and is a string of text.\\n\n",
    "            The model prediction is denoted as ”Output:” and can be one out of three classes [politics, leisure, science].\\n\n",
    "            The confidence_score is denoted as ”Confidence:” and is a value between 0 and 1. A value closer to 1 indicates that the model is more confident in its prediction.\\n\n",
    "\n",
    "            Generate an explanation of the model prediction.\\n\n",
    "            Keep it short and structure it in a way that is easily comprehensible.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the NLEs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide rationeles in prompt or not\n",
    "if params['rationales']['include'] == \"yes\":                # Add both? \n",
    "    column_name = 'prompt'\n",
    "    instruction = instruction_rationale\n",
    "    new_column_name = \"NLE_with_rationale\"\n",
    "\n",
    "elif params['rationales']['include'] == \"no\":\n",
    "    column_name = 'prompt_no_rationale'\n",
    "    instruction = instruction_no_rationale\n",
    "    new_column_name = \"NLE_no_rationale\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Loop and generate NLEs  \n",
    "NLEs = []\n",
    "for index, row in prompts.iterrows():\n",
    "    prompt = row[column_name]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages =[\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": prompt}],    \n",
    "          temperature = params['LLM']['temperature'],\n",
    "          max_tokens = params['LLM']['max_tokens'],\n",
    "          top_p = params['LLM']['top_p'],\n",
    "          frequency_penalty = params['LLM']['frequency_penalty'],\n",
    "          presence_penalty = params['LLM']['presence_penalty'],\n",
    "          seed=42\n",
    "    )  \n",
    "\n",
    "    assistant_respone = response.choices[0].message.content\n",
    "\n",
    "    NLEs.append(assistant_respone.strip(\"\\n\").strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add NLEs to DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use suiting name for column\n",
    "prompts[new_column_name] = NLEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts.to_csv('NLEs_user_study.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
