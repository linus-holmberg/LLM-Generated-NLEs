{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import generated explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file\n",
    "filename = 'NLEs_FaithfulnessStudy.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(word_set):\n",
    "    cleaned_set = set(re.sub(r\"[^a-zA-Z0-9]\", \"\", word.lower()) for word in word_set)\n",
    "    return cleaned_set\n",
    "\n",
    "def calculate_metrics_and_accuracy(row):\n",
    "    # Preparing the sets\n",
    "    NLE_words_rationale = clean_words(set(row['NLE_with_rationale'].lower().split())) # Comparison set\n",
    "    NLE_words_no_rationale = clean_words(set(row['NLE_no_rationale'].lower().split())) # Comparison set\n",
    "    \n",
    "    words_list = ast.literal_eval(row['pred_rationales_words'])\n",
    "    words_list = clean_words(set(word.lower() for word in words_list)) # Ground Truth \n",
    "    \n",
    "\n",
    "    # For rationale\n",
    "    TP_rationale = len(words_list.intersection(NLE_words_rationale))\n",
    "    FP_rationale = len(NLE_words_rationale) - TP_rationale\n",
    "    FN_rationale = len(words_list) - TP_rationale\n",
    "\n",
    "    \n",
    "    precision_rationale = TP_rationale / (TP_rationale + FP_rationale) if (TP_rationale + FP_rationale) > 0 else 0\n",
    "    recall_rationale = TP_rationale / (TP_rationale + FN_rationale) if (TP_rationale + FN_rationale) > 0 else 0\n",
    "    F1_rationale = (2 * precision_rationale * recall_rationale) / (precision_rationale + recall_rationale) if (precision_rationale + recall_rationale) > 0 else 0\n",
    "    accuracy_rationale = TP_rationale / (TP_rationale + FP_rationale + FN_rationale) if (TP_rationale + FP_rationale + FN_rationale) > 0 else 0\n",
    "    \n",
    "    # For no rationale\n",
    "    TP_no_rationale = len(words_list.intersection(NLE_words_no_rationale))\n",
    "    FP_no_rationale = len(NLE_words_no_rationale) - TP_no_rationale\n",
    "    FN_no_rationale = len(words_list) - TP_no_rationale\n",
    "    \n",
    "    precision_no_rationale = TP_no_rationale / (TP_no_rationale + FP_no_rationale) if (TP_no_rationale + FP_no_rationale) > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "    recall_no_rationale = TP_no_rationale / (TP_no_rationale + FN_no_rationale) if (TP_no_rationale + FN_no_rationale) > 0 else 0\n",
    "    F1_no_rationale = (2 * precision_no_rationale * recall_no_rationale) / (precision_no_rationale + recall_no_rationale) if (precision_no_rationale + recall_no_rationale) > 0 else 0\n",
    "    accuracy_no_rationale = TP_no_rationale / (TP_no_rationale + FP_no_rationale + FN_no_rationale) if (TP_no_rationale + FP_no_rationale + FN_no_rationale) > 0 else 0\n",
    "    \n",
    "    return precision_rationale, recall_rationale, F1_rationale, accuracy_rationale, precision_no_rationale, recall_no_rationale, F1_no_rationale, accuracy_no_rationale\n",
    "\n",
    "# Applying the function and assigning the results to new columns\n",
    "results = df.apply(calculate_metrics_and_accuracy, axis=1)\n",
    "df[['precision_rationale', 'recall_rationale', 'F1_rationale', 'accuracy_rationale', 'precision_no_rationale', 'recall_no_rationale', 'F1_no_rationale', 'accuracy_no_rationale']] = pd.DataFrame(results.tolist(), index=df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall average for precision_rationale is: 0.08929025303222575 (Std=0.018691945218619253)\n",
      "___\n",
      "The overall average for recall_rationale is: 0.9705555555555554 (Std=0.07297902681224139)\n",
      "___\n",
      "The overall average for F1_rationale is: 0.16301102645603405 (Std=0.031413930598948814)\n",
      "___\n",
      "The overall average for accuracy_rationale is: 0.08905690939849895 (Std=0.01876971463176385)\n",
      "___\n",
      "The overall average for precision_no_rationale is: 0.1161111111111111 (Std=0.14979997434643783)\n",
      "___\n",
      "The overall average for recall_no_rationale is: 0.014479128913588019 (Std=0.01854177976717904)\n",
      "___\n",
      "The overall average for F1_no_rationale is: 0.02565966725161319 (Std=0.0328344908915196)\n",
      "___\n",
      "The overall average for accuracy_no_rationale is: 0.013279552663983231 (Std=0.017131671662237436)\n",
      "___\n"
     ]
    }
   ],
   "source": [
    "calculate_mean = ['precision_rationale', 'recall_rationale', 'F1_rationale', 'accuracy_rationale', 'precision_no_rationale', 'recall_no_rationale', 'F1_no_rationale', 'accuracy_no_rationale']\n",
    "\n",
    "for metric in calculate_mean:\n",
    "    average_metric = df[str(metric)].mean()\n",
    "    std_metric = df[str(metric)].std()\n",
    "    print(f'The overall average for {metric} is: {average_metric} (Std={std_metric})')\n",
    "    print('___')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Control of Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'NLEs_FaithfulnessStudy.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to select the rationale column based on the prediction for easier comparison\n",
    "def select_rationale(row):\n",
    "    if row['pred'] == 'politics':\n",
    "        return row['formatted_politics_rationales']\n",
    "    elif row['pred'] == 'science':\n",
    "        return row['formatted_science_rationales']\n",
    "    else:\n",
    "        return row['formatted_leisure_rationales']\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df['selected_rationale'] = df.apply(select_rationale, axis=1)\n",
    "\n",
    "# Select only the required columns\n",
    "filtered_df = df[['NLE_with_rationale', 'NLE_no_rationale', 'selected_rationale']]\n",
    "\n",
    "# If you also need to randomly select 50 instances from this filtered DataFrame:\n",
    "sampled_filtered_df = filtered_df.sample(n=50, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_filtered_df.to_csv('Qualitative_Assessment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
